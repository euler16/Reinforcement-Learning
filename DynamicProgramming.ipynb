{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import scipy # FrozenLake needs it apparently\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "(1, 0.0, False, {'prob': 0.3333333333333333})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 5, 0, True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing and testing the gym environment\n",
    "\n",
    "env =  gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "print(env.action_space) \n",
    "print(env.observation_space)\n",
    "\n",
    "for _ in range(1):\n",
    "    env.render()\n",
    "    state = env.step(env.action_space.sample())\n",
    "    print(state)\n",
    "    \n",
    "env.env.P[0][0]\n",
    "env.env.P[5][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.step() returns (next_state, reward, done, probability)<br/>\n",
    "env.env.P\\[state\\]\\[action\\] returns the possible next states which can be achieved <br>\n",
    "\n",
    "### Action Space\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "Implementing policy evaluation in Numpy\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s\\prime}p(s,a,s\\prime)\\bigg[r(s,a,s\\prime) + \\gamma v_{\\pi}(s\\prime) \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "## Algorithm\n",
    "1. Input $\\pi$, the policy to be evaluated\n",
    "2. Initialize an array $v(s) = 0$ , for all $s\\in S^{+}$\n",
    "3. Repeat\n",
    "4. &nbsp;&nbsp;    Initialize $\\Delta \\gets 0$ \n",
    "5. &nbsp;&nbsp;    For each $s \\in S$ do:\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;$temp \\gets v(s)$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;$v(s) \\gets \\sum_{a}\\pi(a|s) \\sum_{s\\prime}p(s,a,s\\prime)\\bigg[r(s,a,s\\prime) + \\gamma v(s\\prime) \\bigg]$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\gets max(\\Delta,|temp - v(s)|)$\n",
    "9. until $\\Delta < \\theta$ (a small positive number)\n",
    "10.Output $v \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env,policy,gamma = 1.0,theta = 1e-5):\n",
    "    '''\n",
    "        policy is a 2D numpy matrix\n",
    "        policy.shape = (number of states, number of actions)\n",
    "        gamma = discount factor\n",
    "        theta = tolerance\n",
    "        env = env.env\n",
    "    '''\n",
    "    v = np.zeros(env.nS) # env.nS = number of states\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            temp = v[s]\n",
    "            tot_val = 0\n",
    "            for a in range(env.nA):\n",
    "                action_val = 0\n",
    "                transition = env.P[s][a]\n",
    "                for trans_prob, next_state, reward, done  in transition:\n",
    "                    action_val += trans_prob*(reward + gamma*v[next_state])    \n",
    "                tot_val += policy[s,a]*action_val\n",
    "            v[s] = tot_val\n",
    "            delta = max(delta, np.abs(temp-v[s]))\n",
    "        # print(delta)\n",
    "        if delta < theta:\n",
    "            complete = True\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01235611 0.01042444 0.01933842 0.00947774 0.01478704 0.\n",
      " 0.03889445 0.         0.03260247 0.08433764 0.13781085 0.\n",
      " 0.         0.17034482 0.43357944 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Deterministic policy\n",
    "det_policy = np.array([[0,0,1,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,0,0,1],\n",
    "                   [0,0,1,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "\n",
    "random_policy = np.ones((env.env.nS,env.env.nA))/env.env.nA # all actions equally probable in all states\n",
    "print(policy_evaluation(env.env,random_policy,0.99,1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "    v = np.zeros(env.nS)\n",
    "    policy = np.random.rand(env.nS,env.nA)\n",
    "    \n",
    "    v = policy_evaluation(env,policy)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
