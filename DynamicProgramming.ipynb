{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import scipy # FrozenLake needs it apparently\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "(1, 0.0, False, {'prob': 0.3333333333333333})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 5, 0, True)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing and testing the gym environment\n",
    "\n",
    "env =  gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "print(env.action_space) \n",
    "print(env.observation_space)\n",
    "\n",
    "for _ in range(1):\n",
    "    env.render()\n",
    "    state = env.step(env.action_space.sample())\n",
    "    print(state)\n",
    "    \n",
    "env.env.P[0][0]\n",
    "env.env.P[5][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.step() returns (next_state, reward, done, probability)<br/>\n",
    "env.env.P\\[state\\]\\[action\\] returns the possible next states which can be achieved <br>\n",
    "\n",
    "### Action Space\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "Implementing policy evaluation in Numpy\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = \\sum_{a}\\pi(a|s) \\sum_{s\\prime}p(s,a,s\\prime)\\bigg[r(s,a,s\\prime) + \\gamma v_{\\pi}(s\\prime) \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "## Algorithm\n",
    "1. Input $\\pi$, the policy to be evaluated\n",
    "2. Initialize an array $v(s) = 0$ , for all $s\\in S^{+}$\n",
    "3. Repeat\n",
    "4. &nbsp;&nbsp;    Initialize $\\Delta \\gets 0$ \n",
    "5. &nbsp;&nbsp;    For each $s \\in S$ do:\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;$temp \\gets v(s)$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;$v(s) \\gets \\sum_{a}\\pi(a|s) \\sum_{s\\prime}p(s,a,s\\prime)\\bigg[r(s,a,s\\prime) + \\gamma v(s\\prime) \\bigg]$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\gets max(\\Delta,|temp - v(s)|)$\n",
    "9. until $\\Delta < \\theta$ (a small positive number)\n",
    "10.Output $v \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env,policy,gamma = 1.0,theta = 1e-5):\n",
    "    '''\n",
    "        policy is a 2D numpy matrix\n",
    "        policy.shape = (number of states, number of actions)\n",
    "        gamma = discount factor\n",
    "        theta = tolerance\n",
    "        env = env.env\n",
    "    '''\n",
    "    v = np.zeros(env.nS) # env.nS = number of states\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            temp = np.copy(v[s])\n",
    "            tot_val = 0\n",
    "            for a in range(env.nA):\n",
    "                action_val = 0\n",
    "                transition = env.P[s][a]\n",
    "                for trans_prob, next_state, reward, done  in transition:\n",
    "                    action_val += trans_prob*(reward + gamma*v[next_state])    \n",
    "                tot_val += policy[s,a]*action_val\n",
    "            v[s] = tot_val\n",
    "            delta = max(delta, np.abs(temp-v[s]))\n",
    "        # print(delta)\n",
    "        if delta < theta:\n",
    "            complete = True\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01235611 0.01042444 0.01933842 0.00947774 0.01478704 0.\n",
      " 0.03889445 0.         0.03260247 0.08433764 0.13781085 0.\n",
      " 0.         0.17034482 0.43357944 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Deterministic policy\n",
    "det_policy = np.array([[0,0,1,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,0,0,1],\n",
    "                   [0,0,1,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,1,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "\n",
    "random_policy = np.ones((env.env.nS,env.env.nA))/env.env.nA # all actions equally probable in all states\n",
    "print(policy_evaluation(env.env,random_policy,0.99,1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "Method for iteratively improving policy\n",
    "## Algorithm\n",
    "1. Initialization $v$ and $\\pi(s) \\in A(s)$ for all $s \\in S$\n",
    "2. Policy evaluation\n",
    "3. Policy Improvement\n",
    "4. If Policy is stable go to Step 2\n",
    "\n",
    "### Policy Improvement\n",
    "1. $policy_stable \\gets true$\n",
    "2. for each $s \\in S$:\n",
    "3. &nbsp;&nbsp; $temp \\gets \\pi(s)$\n",
    "4. &nbsp;&nbsp; $\\pi(s) \\gets argmax_{a}\\sum_{s\\prime}p(s\\prime|s,a)\\bigg[r(s,a,s\\prime)+\\gamma v(s\\prime)\\bigg]$\n",
    "5. If $temp \\neq \\pi(s)$, then $policy_stable \\gets False$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,gamma=1,theta=1e-8):\n",
    "    '''\n",
    "        outputs a deterministic policy\n",
    "    '''\n",
    "    # count = 100\n",
    "    policy = np.ones((env.nS,env.nA))/env.nA\n",
    "    while True:\n",
    "        \n",
    "        v = policy_evaluation(env,policy,gamma,theta)\n",
    "        # policy improvement\n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "\n",
    "            current_action = np.argmax(policy[s])\n",
    "            action_val = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                transition = env.P[s][a]\n",
    "                for trans_prob, next_state, reward, done in transition:\n",
    "                    action_val[a] += trans_prob*(reward + gamma*v[next_state])\n",
    "            best_action = np.argmax(action_val)\n",
    "            \n",
    "            if current_action != best_action:\n",
    "                policy_stable = False\n",
    "                \n",
    "            policy[s] = np.eye(env.nA)[best_action]\n",
    "            \n",
    "        if policy_stable:\n",
    "            break\n",
    "        # count -= 1\n",
    "            \n",
    "    return policy,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0.54202581 0.49880303 0.4706955  0.4568515  0.55845085 0.\n",
      " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
      " 0.         0.7417204  0.86283741 0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_iteration(env.env,0.99)\n",
    "print(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[0][np.argmax(policy[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "10\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "6\n",
      "0\n",
      "Episode finished after 27 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    # print(observation)\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = np.argmax(policy[observation])\n",
    "        print(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "## Algorithm\n",
    "1. Repeat\n",
    "2. &nbsp;&nbsp; $\\Delta \\gets 0$\n",
    "3. &nbsp;&nbsp; for each $s \\in S$:\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp; $temp \\gets v(s)$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp; $v(s) \\gets max_{a}\\sum_{s\\prime}p(s\\prime | s,a)\\bigg[r(s,a,s\\prime) + \\gamma v(s\\prime)\\bigg]$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp; $\\Delta \\gets max(\\Delta,|temp - v(s)|)$\n",
    "7. until $\\Delta < \\theta$\n",
    "\n",
    "Output a deterministic policy $\\pi$, such that <br>\n",
    "$\\pi(s) = argmax_{a}\\sum_{s\\prime}p(s\\prime|s,a)\\bigg[r(s,a,s\\prime) + \\gamma v(s\\prime)\\bigg]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env,gamma=1,theta=1e-8):\n",
    "    \n",
    "    v = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            temp = np.copy(v[s])\n",
    "            action_val = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                transition = env.P[s][a]\n",
    "                for trans_prob, next_state, reward, done in transition:\n",
    "                    action_val[a] += trans_prob*(reward + gamma*v[next_state])\n",
    "            v[s] = np.max(action_val)\n",
    "            delta = max(delta,np.abs(temp-v[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros((env.nS,env.nA))\n",
    "    action_val = np.zeros(env.nA)\n",
    "    for s in range(env.nS):\n",
    "        for a in range(env.nA):\n",
    "            transition = env.P[s][a]\n",
    "            for trans_prob, next_state, reward, done in transition:\n",
    "                action_val[a] += trans_prob*(reward + gamma*v[next_state])\n",
    "        policy[s] = np.eye(env.nA)[np.argmax(action_val)]\n",
    "    \n",
    "    return policy,v\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[0.54202581 0.49880303 0.47069551 0.4568515  0.55845085 0.\n",
      " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
      " 0.         0.7417204  0.86283741 0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy,v = value_iteration(env.env,gamma=0.99)\n",
    "print(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a discussion on difference between value iteration and policy iteration visit [this](https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
